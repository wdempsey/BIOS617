---
title: "BIOS 617 - Lecture 1"
author: "Walter Dempsey"
date: "1/4/2023"
output:
  beamer_presentation: default
  ioslides_presentation:
    css: styles.css
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## BIOS 617 slides in R Markdown

All course slides will be written as R Markdown presentations. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. This lets me share slides and code all in one document making all lecture content reproducible!

PDF copies are available on **[canvas](https://umich.instructure.com/courses/494883/modules)** while the RMD files and data will be available on [github](https://github.com/wdempsey/BIOS617).

## A quick survey while we go over syllabus

- [Syllabus](https://umich.instructure.com/courses/494883/pages/course-syllabus)
- [Quick survey](https://forms.gle/DnoCjZ897s24QKKu7)

## Why surveys? 

- A lucid statement of the objectives is most helpful
- Even when stated forthright, it is easy to forget
- Many design decisions lead to complex surveys with muddled objectives
- **Brexit survey**: 
    + ``Assess public opinion on Remain/Leave debate'' 
- **Pew 2008 US election**:
    + ``Assess public opinion on the national election candidates''
- **National survey for family growth** (NSFG)
    + ``Gathers information on US family life, marriage and divorce, pregnancy, infertility, use of contraception, and general and reproductive health''
    
## Historical perspective: Why probability sampling?

* Statisticians receive data to analyze
    + How is it collected?
    + Independence is a common assumption that makes likelihood easy to compute, but is it reasonable?
    + All sampled elements equally likely to be sampled?
* Historical strategies involved non-probability sampling
    + Purposive samples: Choose both average and extreme elements to cover range of distribution
    + Judgement samples: Experts sample based on judgement of representativeness
    + Convenience samples: Volunteers, street/mall intercepts
* No guarantee that any of these are *representative*

## Population definition

- *Representative* is an ill-defined concept
- We typically mean sample is representative **of a population**
- *Population* denotes the aggregate from which the sample is chosen
  + US adults 18+
  + Family practice physicians licensed in Michigan
  + All currently Ann Arbor residents
- **Superpopulation** denotes hypothetical infinite population from which the population is drawn
- *Sampled* population should coincide with the *target* population

## Sampling frame

- Divide the *population* into indivisible parts
- **Sampling units** (or **units**) cover population and are non-overlapping
- List of sampling units is called a **sampling frame**
    + Can be a major practical problem (incomplete or duplication)
- Units may be have higher-level grouping structure
    + Households, schools 
- Used to obtain sample from the (finite) population of interest

## Probability sampling definition

- Given sampling protocol, theoretically define **all possible samples** that can be obtained
- Each sample has a probability of selection
- Chosen sample is selected via the **randomized** sampling protocol
- Every element in the population has a non-zero probability of selection
    + Estimate population quantities of interest accurately ``on average'' (little or no bias)
    + Estimate uncertainty in population quantity estimates (accurate nominal coverage of confidence intervals)

```{r brexit, echo = FALSE, include = FALSE}
# install packages
if(!require('dslabs')){install.packages('dslabs')}
if(!require('tidyverse')){install.packages('tidyverse', dependencies = TRUE)}
if(!require('ggrepel')){install.packages('ggrepel')}
if(!require('matrixStats')){install.packages('matrixStats')}

# load libraries
library(dslabs)
library(tidyverse)
library(ggrepel)
library(matrixStats)

# set colorblind-friendly color palette
colorblind_palette <- c("black", "#E69F00", "#56B4E9", "#009E73",
                        "#CC79A7", "#F0E442", "#0072B2", "#D55E00")
data(brexit_polls)
```

## Example 1: Pew 2008 opinion poll 

```{r pew, echo = FALSE}
library(foreign)
data <- read.dta("data/pew_research_center_june_elect_wknd_data.dta")
data <- data[!is.na(data$ideo ),]
data <- data[!is.element(data$ideo, c("missing/not asked", "dk/refused")),]
data$ideo_numeric = (data$ideo == 'very conservative')*2 + (data$ideo == 'conservative')*1 + (data$ideo == 'moderate')*0 + (data$ideo == 'liberal')*(-1) + (data$ideo == 'very liberal')*(-2)

data %>%  ggplot(aes(ideo_numeric)) +
  geom_bar(aes(y = (..count..)/sum(..count..)), data=subset(data,survey == 'sept08forpoli'), fill = "blue", alpha = 0.2) +
  geom_bar(aes(y = (..count..)/sum(..count..)), data=subset(data,survey == 'electwkend08'), fill = "red", alpha = 0.2) +
  geom_bar(aes(y = (..count..)/sum(..count..)), data=subset(data,survey == 'june08voter'), fill = "green", alpha = 0.2) +
  xlab("Ideological buckets") +
  ylab("Frequency") +
  labs(color = "Ideology type") +
  ggtitle("Ideology response from 3 Pew 2008 surveys")
```

## Example 2: Brexit data
```{r cars, echo = FALSE, message = FALSE, fig.align='center'}
# plot of Brexit referendum polling spread between "Remain" and "Leave" over time
brexit_polls %>%
  ggplot(aes(enddate, spread, color = poll_type)) +
  geom_hline(aes(yintercept = -.038, color = "Actual spread")) +
  geom_smooth(method = "loess", span = 0.4) +
  geom_point() +
  scale_color_manual(values = colorblind_palette[1:3]) +
  xlab("Poll end date (2016)") +
  ylab("Spread (Proportion Remain - Proportion Leave)") +
  labs(color = "Poll type") +
  ggtitle("Spread of Brexit referendum online and telephone polls")
```

## Discussion

- What is the population for the Brexit survey? 
- What is the sampling frame?
- What are potential practical issues with defining the frame?

## Survey sampling notation: population

- Number of elements: $N$
- Values of variable: $Y_1, \ldots, Y_N$
- Population total: $Y = \sum_{i=1}^N Y_i$
- Population mean: $\bar Y = Y/N$
- Population variance: $S^2 = (N-1)^{-1} \sum_{i=1}^N (Y_i - \bar Y)^2$
- Sampling indicator: $I_1, \ldots, I_N$


## Survey sampling notation: sample

- Number of elements: $n$
- Values of variable: $y_1, \ldots, y_n$
- Sample total: $y = \sum_{i=1}^n y_i$
- Sample mean: $\bar y = y/n$
- Sample variance: $s^2 = (n-1)^{-1} \sum_{i=1}^n (y_i - \bar y)^2$
- Sampling fraction: $f = n/N$
- When $n = N$ , sample = population

## Design-based inference

Also known as **randomization-based** inference.

- *Model-based*: assume a model for the distribution of the data in the population
- *Design-based*: condition on the values $Y_1, \ldots, Y_n$. 
    + Inference is with respect to the distribution of $\{ I_j \}_{j=1}^N$
    + Over all possible samples $s = 1,\ldots, S$   under a given probability sampling design.
    

## Design-based inference: Simple example

```{r heights, echo = FALSE, fig.align='center'}
set.seed(1)
heights = rnorm(n = 35, mean = 65, sd = 5)
heights = data.frame(heights)
heights %>%
  ggplot(aes(heights)) + geom_histogram(binwidth = 1) +  
  xlab("Student height (in inches)") +
  ylab("Count") +
  ggtitle("Simulated height of students in BIOS617")
```

## Design-based inference: Simple example
```{r sampled, echo = FALSE}
set.seed(179)
swor = sample(1:35, size = 10, replace = FALSE)
swor_result = mean(heights$heights[swor])
swr = sample(1:35, size = 10, replace = TRUE)
swr_result = mean(heights$heights[swr])
```

* Population's average height is `r round(mean(heights$heights),4)`
* Sampling procedures: sample 10 individual's with/without replacement
* Estimate of mean from given sample? $\sum_j I_j Y_j / 10$
* Example sample w/o replacement: `r swor`
* Corresponding estimate: `r round(swor_result,4)`
* Example sample w replacement: `r swr`
* Corresponding estimate: `r round(swr_result,4)`
* Are these good estimators?  Are they the same?
* Intuitively, which is a better procedure? Why?

## Review of expectations
* Continuous variable $X$ then $\mu_x = E[X]=\int x f_x(x) dx$. 
* Linearity of expectations,
$$ 
E [ a X ] = \int a x f_x (x) dx = a \int x f_x (x) dx = a E[ X ] 
$$
* Two continuous variables $X$ and $Y$, then
$$ 
\begin{aligned}
E[ X + Y ] &= \int \int (x+y) f_{xy} (x,y) dx dy \\
&= \int \int x f_{xy} (x,y) dx dy + \int \int y f_{xy} (x,y) dx dy \\
&= \int x \left[ \int f_{xy} (x,y) dy \right] dx + \int y \left[ \int f_{xy} (x,y) dy \right] dx
\end{aligned}
$$

## Review of expectations
* Two continuous variables $X$ and $Y$, then
$$ 
\small \begin{aligned}
E[ X + Y ] &= \int x f_{x} (x) dx + \int y f_{y} (y) dy = E[X] + E[Y].
\end{aligned}
$$
So $\small E \left[ \sum_j a_j X_j \right] = \sum_j a_j E \left[ X_j \right]$.
* If $X$ and $Y$ independent, then
$$ 
\small \begin{aligned}
E[X \cdot Y ] &= \int \int x y f_{xy} (x,y) dy dx \\
&= \int x f_x (x)  \left[ \int y f_y (y) dy \right] dx \\
&= E[Y] \int x f_x (x) dx = E[X] \cdot E[Y].
\end{aligned}
$$

## Review of variances
* For $X$ and $Y$, the variance is 
$$ 
\small {\text Cov} (X, Y) = E [ (X-\mu_x ) (Y - \mu_y) ] = {\text Cov} (Y,X)
$$ and $V(X) = Cov(X,X)$.
* For random variables $\{ X_j \}_{j=1}^k$, 

$$ \small
V \left(\sum_j a_j X_j \right) = \sum_j a_j^2 V(X_j) + 2 \sum_j \sum_{i < j} a_i a_j \text{Cov}(X_i, X_j) 
$$

* Corollary: If $\{ X_j \}$ are independent, then 
$$ 
\small V \left(\sum_j a_j X_j \right) = \sum_j a_j^2 V(X_j) 
$$

## Review of variances (derivation)

A quick derivation: $V \left(\sum_j a_j X_j \right)=$

$$
\small 
\begin{aligned} 
 &= E \left[ \left( \sum_j a_j (X_j - \mu_j) \right)^2\right] \\
&= E \left[ \sum_{i,j=1}^n a_j a_i (X_j - \mu_j) (X_i - \mu_i) \right] \\
&= E \left[ \sum_{j} a_j^2 (X_j - \mu_j)^2 \right] + E \left[ \sum_j \sum_{i \neq j} a_i a_j (X_j - \mu_j) (X_i - \mu_i) \right] \\
&= \sum_{j} a_j^2 E \left[ (X_j - \mu_j)^2 \right] + \sum_j \sum_{i \neq j} a_i a_j E \left[ (X_j - \mu_j) (X_i - \mu_i) \right]\\
&= \sum_{j} a_j^2 V(X_j) + \sum_j \sum_{i \neq j} a_i a_j \text{Cov}(X_i, X_j)
\end{aligned}
$$

## Iterated expectation and variance

Consider a set of mutually exclusive and exhaustive conditions $C_j$. For simplicity, assume $X$ is discrete. Then
$$
P(X = x_k) = \sum_j P(X_k \cap C_j) = \sum_j P(X_k \, | \, C_j) P(C_j)
$$
Then
$$
\small
\begin{aligned}
E[X] &= \sum_k x_k P(X = x_k) \\
&= \sum_k \left[\sum_j x_k P(X = x_k \, | \, C_j) \right] P(C_j) \\
&= \sum_k E_{x} \left[ X \, | \, C \right] P(C_j) \\
&= E_{C} \left[ E_{X} \left[ X \, | \, C \right] \right]
\end{aligned}
$$

## Iterated expecation and variance

$$
\small \begin{aligned}
V(X) &= E \left[ (X-\mu)^2 \right] = E_{C} \left[ E_{X} \left[ (X-\mu)^2 \, | \, C \right] \right] \\
&= E_{C} \left[ E_{X} \left[ (X- E[X \, | \, C] + E[X \, | \, C] - \mu)^2 \, | \, C \right] \right] \\
&= E_{C} \left[ E_{X} \left[ (X- E[X \, | \, C])^2 \, | \, C \right] \right] \\
&+ 2 \cdot E_{C} \left[ E_{X} \left[ (X- E[X \, | \, C]) \times (E[X \, | \, C] - \mu) \, | \, C \right] \right] \\
&+ E_{C} \left[ E_{X} \left[ (E[X \, | \, C] - \mu)^2 \, | \, C \right] \right] \\
&= E_{C} \left[ V_{X \, | \, C} (X) \right] + V_{C} ( E_{X \, | \, C} (X) )
\end{aligned}
$$

## Expectation and variance in design-based inference

For a given sample $s$ :

* $t_s$ : statistic of interest computed from sample values
* $P_s$ : probability of sample $s$ being selected
* $T$ : population parameter of interest

We hope that $E[ t_s ] = \sum_s P_s \times t_s = T$ 

We also hope variance is small: $V(t_s) = E[ (t_s - E[t_s])^2]$

## Height example

We had $t_s = \sum_j I_j Y_j / n$ where $s$ is any set of size $n$.

If we sample without replacement, the probability of a given sample is
$$ P_s = \frac{1}{{N \choose n}} = \frac{(N-n)! n!}{N!} $$

The probability that $I_j = 1$ can be shown equal to $n/N$. Then
$$
E[t_s] = \sum_s P_s t_s = (1/n) \times \sum_j \frac{n}{N} Y_j = \bar Y
$$



