---
title: "BIOS 617 - Lecture 6"
author: "Walter Dempsey"
date: "1/30/2022"
output:
  beamer_presentation: default
  ioslides_presentation: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

 Subdomain estimation 

* Stratified sample: know $N$, $N_h$ for $h=1,\ldots, H$
* We are interested in a subdomain analysis using the stratified sample.
* $M_h$: number of subdomain elements in stratum $h$
* $M = \sum_h M_h$
* If known, then estimator is similar estimator to population average
$$
\bar Y = \sum_h \left( \frac{N_h}{N} \right) \bar Y_h \to \bar Y = \sum_h \left( \frac{M_h}{M} \right) \bar Y_h
$$
where $\bar Y_h$ is the subdomain estimator from previous lecture.
* If unknown, then need to estimate $\hat M_h$

## How to estimate $M_h$?

* $m_h \leq n_h$: number of observed individuals in subdomain
* Stratified sampling: $N_h$, $N$, $n_h$, $n$ all __known__ and __fixed__; $M_h$ __unknown__ and __fixed__
* Stratified sampling: We __observe__ $m_h$ in subdomain in the sample $n_h$ but it is __random__
* Key idea: 
$$E \left[ m_h \right] = \frac{M_h}{N_h} \cdot n_h,$$  
so then an unbiased estimator is given by
$$\hat M_h = \frac{N_h}{n_h} m_h$$
* Easily extend to estimator for $M$: $\hat M = \sum_h \hat M_h$

## How to esimate $\bar y$?

* Estimator is given by
$$
\bar y = \frac{ \sum_h \hat M_h \bar y_h}{\sum_h \hat M_h } = \frac{ \sum_h (N_h/n_h) m_h \bar y_h}{\sum_h (N_h / n_h) m_h }
= \frac{ \sum_h (N_h/n_h) \sum_{i=1}^{m_h} y_{hi}}{\sum_h (N_h / n_h) m_h }
$$
is an unbiased estimator of $\bar Y$.

* If design is proportionate, then $n_h / N_h \equiv f$, and 

$$
\bar y_s = \frac{f^{-1}\sum_{h=1}^H \sum_{i=1}^{m_h} y_{hi} }{f^{-1} \sum_h m_h } = \frac{1}{m} \sum_{h=1}^H \sum_{i=1}^{m_h} y_{hi}
$$
the simple subdomain mean.

## What is the variance?

* Let $z_{hi} = \delta_{hi} (y_{hi} - \bar Y_s)$ where $\delta_{hi}$ is 1 if the ith element is in the subdomain and $0$ otherwise 
* By definition, $m_h = \sum_i \delta_{hi}$ and $m = \sum_h \sum_i \delta_{hi}$; then

$$
\bar y_s - \bar Y = \frac{1}{m} \sum_{h=1}^H \sum_{i=1}^{{\bf n_h}} z_{hi}
$$

* Simple approach: replace $m$ with its expectation $n M / N$ yields

$$
\bar y_s - \bar Y = \frac{N}{nM} \sum_{h=1}^H \sum_{i=1}^{n_h} z_{hi} = \frac{N \bar z}{M}
$$

## Variance calculation (ctd)

* Subtracting constant makes computation simple:
$$
\begin{aligned}
V(\bar y_s) &= V(\bar y_s - \bar Y) = \frac{N^2}{M^2} (1-f) \sum_h P_h S_{h(z)}^2 / n
\end{aligned}
$$
where
$$
\begin{aligned}
S_{h(z)}^2 &= \left( \sum_{i=1}^{N_h} Z_{hi}^2 - \left( \sum_{i=1}^{N_h} Z_{hi} \right)^2 / N_h \right) / (N_h - 1) \\
&\approx N_h^{-1} \left( \sum_i Z_{hi}^2 - \left( \sum_i Z_{hi} \right)^2 / N_h \right)
\end{aligned}
$$

## Variance calculation (ctd)

$$
\begin{aligned}
V(\bar y_s) &\approx \frac{N^2}{nM^2} (1-f) \sum_h \frac{N_h}{N} \frac{1}{N_h} \left( \sum_i Z_{hi}^2 - \left( \sum_i Z_{hi} \right)^2 / N_h \right)  \\
&= \frac{N}{nM^2} (1-f) \sum_h \left( \sum_i Z_{hi}^2 - \left( \sum_i Z_{hi} \right)^2 / N_h \right)
\end{aligned}
$$

## Variance calculation (ctd)

$$
\begin{aligned}
\sum_h \sum_i Z_{hi}^2 &= \sum_h \sum_i (Y_{hi} - \bar Y )^2 =  \sum_h \sum_i (Y_{hi} - \bar Y_h + \bar Y_h - \bar Y )^2 \\
&= \sum_h \sum_i (Y_{hi} - \bar Y_h )^2  + \sum_h M_h (\bar Y_h - \bar Y)^2 \\
&+ 2 \underbrace{\sum_i \sum_h (Y_{hi} - \bar Y_h) (\bar Y_h - \bar Y)}_{=0}
\end{aligned}
$$

$$
\sum_h \left( \sum_i Z_{hi} \right)^2 / N_h = \sum_h \left( \sum_i (Y_{hi} - \bar Y ) \right)^2 / N_h = 
\sum_h M_h (\bar Y_h - \bar Y)^2 / N_h
$$

## Variance calculation (ctd)

$$
\begin{aligned}
V(\bar y_s) &= \frac{N}{nM^2} \frac{N-n}{N} \bigg ( \sum_h \sum_i (Y_{hi} - \bar Y_h)^2 \\
&+ \sum_h M_h \left( 1 - \frac{M_h}{N_h} \right) ( \bar Y_h - \bar Y )^2 \bigg) \\
&= \frac{1}{M^2} \left[ \frac{N}{n} - 1 \right] \bigg( \sum_h \sum_i (Y_{hi} - \bar Y_h)^2 \\
&+ \sum_h M_h \left( 1 - \frac{M_h}{N_h} \right) ( \bar Y_h - \bar Y )^2 \bigg) 
\end{aligned}
$$


## Variance under proportionate allocation

Under proportionate allocation, $\frac{m}{M} \approx \frac{n}{N}$ and 

$$
\begin{aligned}
V(\bar y_s ) &\approx \frac{1}{M^2} \left[ \frac{M}{m} - 1 \right] \bigg( \sum_h \sum_i (Y_{hi} - \bar Y_h)^2 \\
&+ \sum_h M_h \left( 1 - \frac{M_h}{N_h} \right) ( \bar Y_h - \bar Y )^2 \bigg) \\
&\approx (1-f) \frac{1}{mM} \bigg( \sum_h \sum_i (Y_{hi} - \bar Y_h)^2 \\
&+ \sum_h M_h \left( 1 - \frac{M_h}{N_h} \right) ( \bar Y_h - \bar Y )^2 \bigg) 
\end{aligned}
$$



## Discussion of variance

* The first term is approximately the variance of a proportionate stratified design with $m_h$ elements sampled
* The second term is the loss from the subclass analysis
* If $M_h/N_h \to 1$ the second term approaches $0$ so the variance approximates that of proportionate stratification
* If $M_h/N_h \to 0$ then 

$$
\begin{aligned}
&\sum_h \sum_i (Y_{hi} - \bar Y)^2 + \sum_h M_h \left( 1 - \frac{M_h}{N_h} \right) (\bar Y_h - \bar Y)^2 \\
&\to
\sum_h \sum_i (Y_{hi} - \bar Y)^2 + \sum_h M_h (\bar Y_h - \bar Y)^2 \\
&\sum_h \sum_i (Y_{hi} - \bar Y)^2 + \sum_h M_h (\bar Y_h - \bar Y)^2 =
\sum_i \sum_h (Y_{hi} - \bar Y)^2 = M S^2
\end{aligned}
$$
and thus $V(\bar y_s) \to (1-f) S^2 /m$ which is the SRS variance.

## Equal-size cluster sampling

* Often obtaining a sampling frame containing the individual units of interest may be difficult.
  + There are no listings of US adults; there are, however, listings of household addresses that are approximately complete (and can be made moreso by review, either “on foot” or, where possible, using Google Earth).
  + Similarly, it might be more convenient to list classes in a school rather
than students; nursing homes rather than patients; etc.
* For ease of exposition, we are going to assume for the moment
that
  + Once a cluster is sampled, all of the elements in that clusters are also
sampled (may or may not be practical or efficient).
  + All clusters are of equal size.
  + Clusters are sampled via SRS without replacement.

## Notation

* $i=1,\ldots, K$ clusters in the population
* $i=1,\ldots, k$ clusters in the sample
* $j=1,\ldots, M$ units in each cluster $(M \geq 2$)
* Total number of units in the population: $N=KM$
* Total number of units in the sample: $n=kM$
* $Y_{ij}$ is the observation associated with the $j$th unit in the $i$th cluster
* $\bar Y_i = M^{-1} \sum_j Y_{ij}$ is the mean of the $i$th cluster
* $\bar Y = K^{-1} \sum_i \bar Y_i = (KM)^{-1} \sum_{i=1}^K \sum_{j=1}^M Y_{ij}$

## Example

```{r, out.width = "250px", fig.align='center'}
library(knitr)
include_graphics("./figs/l6_fig1.png") # place holder
```

## Example

```{r, out.width = "250px", fig.align='center'}
library(knitr)
include_graphics("./figs/l6_fig2.png") # place holder
```

## Estimator and variance

* Since all elements in the cluster are observed if a cluster is sampled, $\bar y_i = \bar Y_i$
* I.e., the observed cluster mean is the population mean
* Hence $(\bar y_1, \ldots, \bar y_k)$ can be viewed as an SRS sample from $(\bar Y_1, \ldots, \bar Y_K)$
* So that $\bar y_c = k^{-1} \sum_{i=1}^k \bar y_i = n^{-1} \sum_i \sum_j y_{ij}$ is unbiased for $\bar Y$
with variance 

$$
\frac{(1-f_K) S_K^2}{k}  = \frac{K-k}{K} \times \frac{1}{k} \times \frac{1}{K-1} \sum_i (\bar Y_i - \bar Y)^2
$$

* And $s_k^2 = (k-1)^{-1} \sum_i (\bar y_i - \bar y)^2$ is an unbiased estimator for $S_K^2$

## Design effect and intraclass correlation $\rho$

$$
\begin{aligned}
\sum_i (\bar Y_i - \bar Y)^2 &= \sum_i \left[ \frac{1}{M} \sum_j ( Y_{ij} - \bar Y ) \right]^2 \\
&= \frac{1}{M^2} \left [ \sum_i \sum_j ( Y_{ij} - \bar Y )^2 + \sum_i \sum_{j \neq l} (Y_{ij} - \bar Y) (Y_{il} - \bar Y) \right] \\
&= \frac{1}{M^2} \left [ \underbrace{(KM-1)}_{N-1} S^2 + \sum_i \sum_{j \neq l} (Y_{ij} - \bar Y) (Y_{il} - \bar Y) \right] \\
\end{aligned}
$$


## Intraclass correlation

The finite population between-cluster covariance is given by
$$
E[ (Y_{ij} - \bar Y) (Y_{il} - \bar Y) ] = \frac{1}{KM(M-1)} \sum_i \sum_{j \neq l} (Y_{ij} - \bar Y) ( Y_{il} - \bar Y )
$$
and the _intraclass correlation_ $\rho$ by
$$
\begin{aligned}
\rho &= \frac{E(Y_{ij} - \bar Y)(Y_{il} - \bar Y)}{E (Y_{ij} - \bar Y)^2} \\
&= \frac{\frac{1}{KM(M-1)} \sum_i \sum_{j \neq l} \sum_l (Y_{ij} - \bar Y) (Y_{il} - \bar Y)}{\frac{1}{KM} \sum_i \sum_l (Y_{ij} - \bar Y)^2} \\
&= \frac{\sum_i \sum_{j \neq l} \sum_l (Y_{ij} - \bar Y) (Y_{il} - \bar Y)}{(KM-1)(M-1) S^2} \\
\end{aligned}
$$

## Design effect and intraclass correlation $\rho$

* Re-writing in terms of $\rho$:

$$
\sum_{i=1}^{K} (\bar Y_i - \bar Y)^2 = \frac{1}{M^2} \left[ (KM-1) S^2 + (KM-1)(M-1)S^2 \rho \right]
$$
and 
$$
S_k^2 = \frac{1}{K-1} \sum_{i=1}^K \left( \bar Y_i  - \bar Y \right)^2 = \frac{KM-1}{(K-1)M^2} S^2 \left[ 1 + (M-1) \rho \right]
$$

* Therefore
$$
\begin{aligned}
V(\bar y_c) &= \left(1 - \frac{k}{K} \right) \frac{S_k^2}{k} \\
&=\left(1 - \frac{k}{K} \right) \frac{S^2}{k} \frac{KM-1}{(K-1)M^2} \left[ 1 + (M-1) \rho \right]
\end{aligned}
$$

## Overall variance

$$
\begin{aligned}
V(\bar y_c) &=\left(1 - \frac{k}{K} \right) \frac{S^2}{k} \frac{KM-1}{(K-1)M^2} \left[ 1 + (M-1) \rho \right] \\
&=\left(1 - \frac{k}{K} \right) \frac{S^2}{kM} \frac{KM-1}{(KM-M} \left[ 1 + (M-1) \rho \right] \\
&=\left(1 - \frac{n}{N} \right) \frac{S^2}{n} \frac{KM-1}{KM-M} \left[ 1 + (M-1) \rho \right] \\
&\approx\left(1 - \frac{n}{N} \right) \frac{S^2}{n} \left[ 1 + (M-1) \rho \right] \\
&= V(\bar y_{SRS}) \left[ 1 + (M-1) \rho \right]
\end{aligned}
$$

## Population anova table

* Between clusters:
  + SS: $M \sum_i (\bar Y_i - \bar Y)^2$
  + df: $K-1$
  + MS: $M S_k^2$
* Within clusters
  + SS: $\sum_i \sum_j (Y_{ij} - \bar Y_i)^2$
  + df: $KM-K$
  + MS: $S_w^2$
* Total
  + SS: $\sum_i \sum_j (Y_{ij} - \bar Y)^2$
  + df: $KM-K+K-1 = KM-1$
  + MS: $S^2$
 
## Connection between $S^2$ and $S_w^2$

$$ 
\begin{aligned}
&(KM-1) S^2 = (K-1) M S_k^2 + K(M-1) S_w^2 \\
= &\frac{(K-1)M(KM-1)}{(K-1)M^2} S^2 \left[ 1+ (M-1) \rho \right] + K(M-1) S_w^2 \\
= &\frac{(KM-1)}{M} S^2 \left[ 1+ (M-1) \rho \right] + K(M-1) S_w^2 \\
\Rightarrow  &(KM-1) \left[ 1 - \frac{1 + (M-1)\rho}{M} \right] = K(M-1) S_w^2 \\
\Rightarrow  &\frac{(KM-1)(M-1)}{M} S^2 (1-\rho) = K(M-1) S_w^2 \\
\Rightarrow  &\frac{KM-1}{KM} S^2 (1-\rho) = S_w^2 \\
\Rightarrow  &S^2 (1-\rho) \approx S_w^2
\end{aligned}
$$

## Design effects in equal-size cluster sampling

The deff for equal-size cluster sampling is given by

$$
deff = \frac{V(\bar y_c)}{ V(\bar y_{SRS})} \approx \frac{V(\bar y_{SRS})(1+(M-1)\rho)}{V(\bar y_{SRS})} = [1+(M-1)\rho]
$$

* $deff < 1$ when $\rho < 0$
* $deff > 1$ when $\rho > 0$
* Minimized when $1+ (M-1) \rho = 0$ which is equivalent to $\rho = - \frac{1}{M-1}$
* Maximized when $\rho = 1$: $deff = M$

## Intercorrelation discussion

* The large majority of the time $\rho > 0$, so that cluster sampling is statistically inefficient relative to SRS
  +   Compare this with stratified sampling. Both approaches have units sampled within a larger grouping. Why is cluster sampling less efficient and stratified sampling more efficient than SRS when there is within-group homogeneity?
* Also, even small values of $\rho$ can lead to large design effects if $M$ is large
* If $\rho$ is small but $M$ is large, take smaller within-cluster samples $(m<M)$ and more clusers $k^\prime = \frac{n}{m} > \frac{n}{M} = k$ (will discuss soon).

## Estimating intraclass correlation

* Direct computation of $\rho$ gets difficult in most practical settings, where there are unequal clusters sizes and or three or more stages of sampling.
* However, a synthetic $\rho$ can still be computed as 
$$ \rho = \frac{deff -1}{\bar M - 1}$$
where  $\bar M$ is the mean number of units per the (highest-level) cluster.

## Intraclass correlation and confidence intervals, sample size

* Using $\rho$ a quick confidence interval for a mean can be computed from any cluster:
$$
\bar y_c \pm t_{(0.975, k-1)} \sqrt{[1+(M-1)\rho]s^2/n}
$$

* Using an estimate of $\rho$, sample sizes computed under SRS can be adjusted:
$$
n = [1+(M-1)\rho] n_{SRS}
$$


* If you can compute $\hat V(\bar y_c)$ then use the estimated variance rather than the approximate variance.