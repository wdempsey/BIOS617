---
title: "BIOS 617 - Final Lecture"
author: "Walter Dempsey"
date: "4/20/2020"
output:
  beamer_presentation: default
  ioslides_presentation: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(knitr)
library(survey)
library(pps)
library(expm)
```

# Review day

## Final announcement details

* Exam date and time: Monday April 25, 1:30-3:30pm
* Exam will be posted to Canvas under ``Assignments'' at 1:20pm
* Exam needs to be __uploaded__ (like standard HW) by 3:40pm.
* I will be in class have my Zoom open.  Link will be announced on canvas.
* Please feel free to ask me questions while taking exam.  
* Basic R allowed but no packages.
* E-mail with `BIOS 617` if necessary

## Topic 12:  Unequal cluster sizes

* Introduced general weigted estimator

$$
\bar y_{wc} = \frac{1}{N} \sum_{i=1}^k \sum_{j=1}^{m_i} w_{ij} y_{ij}
$$

* SRS, stratified, poststratified, and cluster sampling all fit within this paradigm for a different choice of weights
  + Name those weights!
* For cluster sampling: $\frac{k}{K} \times \frac{m_i}{M} = \pi_{ij}$

## Topic 12: Variance issues

* Variance is given by
$$
v(\bar y_{wc}) = \frac{K^2}{N^2} \left( 1 - f_1 \right) \frac{s_{Y_i}^2}{k} + \frac{K}{N^2 k} \sum_{i=1}^k M_i^2 \left( 1 - \frac{m_i}{M_i} \right) \frac{s_i^2}{m_i}
$$

* First term involves variance of PSU totals, which are a function of __both__ the cluster __means__ $\bar Y_i$ and the cluster __sizes__ $M_i$
* Typically $M_i$ may be highly variable
* Hence an alternative mean estimator with a reduced variance may be preferable.

$$
\bar y_r = \frac{\sum_{i=1}^k \sum_{j=1}^{m_i} w_{ij} y_{ij} }{\sum_{i=1}^k \sum_{j=1}^{m_i} w_{ij}} 
$$
* What does this look like for cluster sampling?

## Topic 13:  General procedure for calculating variance

* Obtain an unbiased estimator of $V(\tilde y_1)$ under single-stage sampling, say $v^\star (\tilde y)$
* Substitute $\hat Y_{i}$ for $Y_i$ in $v^\star (\tilde y)$
* Add $\sum_{i=1}^k w_{is} v(\hat Y_i \mid i \in s)$ if estimating a total or $N^{-1} \sum_{i=1}^k w_{is} v(\hat Y_i \mid i \in s)$ if estimating a mean

## Topic 14: Probability proportional (PPS) to size sampling

* Select with first stage $\frac{kM_i}{M_0}$
* Select with second stage probability $\frac{m}{M_i}$
* Design is epsem for fixed sample size $n$:
$$
P(I_{ij} = 1) = \pi_{ij} = \pi_i \pi_{j \mid i \in s} 
= \frac{kM_i}{M_0} \times \frac{m}{M_i} = \frac{k m}{M_0} 
= \frac{n}{N} = \pi
$$
for all $i,j$.

## Topic 14: two-stage PPS sampling

* Epsem design $\Rightarrow$ sample mean is unbiased
* Variance is given by 

$$ 
\begin{aligned}
V(\bar y_{pps} ) &= \frac{\sigma_1^2}{k} + \frac{\sigma_2^2}{km} = \frac{\sigma_1^2}{k} + \frac{\sigma_2^2}{n}
\end{aligned}
$$
where
$$
\sigma_1^2 = \frac{1}{N} \sum_{i=1}^K M_i \left( \bar Y_i - \bar Y \right)^2
$$
and
$$
\begin{aligned}
\sigma_2^2 &= \frac{1}{N} \sum_{i=1}^K M_i \sigma_i^2 \\
\sigma_i^2 &= \frac{1}{M_i} \sum_{i=1}^K (Y_{ij} - \bar Y_i )^2
\end{aligned}
$$

## Topic 15: Probability proportional to estimated size

* Assume second stage sampling with constant sample size 
* Joint sampling probability
$$
\pi_{ij} = \frac{k \tilde M_i}{N} \times \frac{m}{M_i} = \frac{n \tilde M_i}{N M_i} = w_{ij}^{-1}
$$

* Weights must now be used to obtain unbiased estimators of means and totals:
$$
\bar y_{ppes} = \frac{\sum_{i=1}^k \sum_{j=1}^m w_{ij} y_{ij} }{\sum_{i=1}^k \sum_{j=1}^m w_{ij}} = 
\frac{\sum_{i=1}^k \frac{M_i}{\tilde M_i} \sum_{j=1}^m y_{ij} }{\sum_{i=1}^k \frac{M_i}{\tilde M_i}  \cdot m} =
\frac{\sum_{i=1}^k w_i \bar y_i}{\sum_{i=1}^k w_i}
$$
where $w_i = \frac{M_i}{\tilde M_i}$.

* Ratio estimation $\rightarrow$ ratio variance calculations

## Topic 16: General Variance Methods

* Method 1: Taylor series

* Using vector notation, our Taylor Series approximation for $V( g( {\bf u}))$ is given by

$$
V( g( {\bf u})) = \left( \frac{\partial g}{\partial {\bf u}} \mid_{{\bf u} = {\bf U}} \right)^\top V( {\bf u}) \left( \frac{\partial g}{\partial {\bf u}} \mid_{{\bf u} = {\bf U}} \right)
$$

## Sandwich estimator

* __Sandwich estimator__: 
  + Bread: Inverse of the derivative of score equation evaluated at $b_w$
  + Meat: variance of the score equation evaluated at $b_w$
* Estimator of $V(b_w)$ now requires an estimator of $v(U_w(b_w))$
+ But we know how to estimate population total for all of the sample designs we have discussed.

## Estimator $v(U_w (b_w))$

We can simplify further by noticing
$$
U_w (b_w) = -\sigma^{-2} \sum_{i=1}^n z_i, \quad z_i = w_i e_i x_i, e_i = y_i - x_i^\top b_w
$$
Then, for a general stratified, clustered design with arbitrary weights,
$$
v(U_w (b_w)) = \sigma^{-4} \sum_{i=1}^h \frac{k_i}{k_i-1} \sum_{j=1}^{k_i} (z_{ij} - \bar z_i) (z_{ij} - \bar z_i)^\top
$$
where $h$ is the stratum, $k_i$ is the number of PSUs in the $i$th stratum, $z_{ij} = \sum_{l=1}^{r_{ij}} z_{ijl}$, and $\bar z_i = \frac{1}{k_i} \sum_{j=1}^{k_i} z_{ij}$. Thus
$$
(X^\prime W X)^{-1} \left( \sum_{i=1}^h \frac{k_i}{k_i-1} \sum_{j=1}^{k_i} (z_{ij} - \bar z_i) (z_{ij} - \bar z_i)^\top \right) (X^\prime W X)^{-1}
$$

## Topic 16: General variance methods

* Method 2: BRR is a method that assumes 2 PSUs per stratum (paired selection model).
* In practice this might not be the case, so approximations are made by collapsing/combining strata
  + Ultimate cluster sampling (ignore lower levels of clustering)
  + With-replacement approximations
  + Creating of Sampling Error Computation Units (SECUs)
* Variance under sampling with replacement:

$$
v_c (y) = \frac{\sum_{c=1}^C ( y^\prime_c - y )^2}{C}
$$

* Without replacement

$$
\begin{aligned}
v_c (y) = \sum_{h=1}^H d_h^2 + 2 \sum_{h=k+1}^H \sum_{k=1}^H \left( \sum_{c=1}^C \frac{\xi_{ch} \xi_{ck}}{C} \right) d_h d_k
\end{aligned}
$$

## Method 3: Jackknife repeated replication

Jackknife estimators of variance can be obtained for any of the general forms of sample design, by either letting $H=1$ in the absence of stratification, or by treating each observation as a PSU in the absence of clustering.

Thus for a clustered design without stratification
$$
v_{JRR} (\hat \theta) = \frac{k-1}{k} \sum_{i=1}^k \left( \hat \theta_{(i)} - \hat \theta \right)^2
$$
where $\hat \theta_{(i)}$ is obtained dropping the $i$th PSU and weighting up the remaining observations $k/(k-1)$

## Bootstrap

The bootstrap, like the jackknife, uses the empirical distribution function to estimate variance. In the SRS setting, the existing sample of n elements in sampled __with replacement__ B times to yield a set of estimators $\{ \hat \theta^{(b)} \}$ for $b=1,\ldots,B$.  The variance of $\hat \theta$  is estimated by the variance of the bootstrap estimators:

$$
v_{boot} (\hat \theta) = (B-1)^{-1} \sum_{b=1}^B \left( \hat \theta^{(b)} - \hat \theta \right)^2 
$$

## Topic 17: 2-phase sampling

* Take a first phase sampling of $n^\prime$ elements from a population via SRS
* Allocate the sampled elements to the $h=1,\ldots,H$ strata
* Sample $n_h \leq n_h^\prime$ elements from the $h$th stratum via SRS
* Estimate is $\bar y_{ds} = \sum_{h=1}^H p_h \bar y_h$ for 
  + $p_h = n_h^\prime / n^\prime$ 
  + $\bar y_h = n_h^{-1} \sum_{i=1}^{n_h} y_{hi}$
* Variance is given by
$$
\begin{aligned}
V(\bar y_{ds} ) &= \sum_{h=1}^H P_h^2 S_h^2 (1-f_h) / n_h+ b \sum_{h=1}^H P_h (1-P_h) S_h^2 (1-f_h)/n_h + \\
&+ b \sum_{h=1}^H P_h ( \bar Y_h - \bar Y)^2, \quad 
b = \frac{N}{N-1} \left( \frac{N-n^\prime}{N} \right) \frac{1}{n^\prime}
\end{aligned}
$$

## Topic 17: 2-phase sampling, optimal allocation

$$
C^\star V \approx \left( c^\prime + \sum_h c_h \nu_h P_h \right) \times \left[ \left( S^2 - \sum_h P_h S_h^2 \right) + \sum_{h=1}^H \frac{P_h S_h^2}{\nu_h} \right]
$$

Plugging $$\nu_h = S_h \left[ \frac{c^\prime}{c_h (S^2 - \sum_h P_h S_h^2)} \right]^{1/2}$$ into $C^\star V$, we obtain
$$
V \approx \frac{1}{C^\star} \left[ \sum_h P_h S_h \sqrt{c_h} +
(S^2 - \sum_h P_h S_h^2 )^{1/2} \sqrt{ c^\prime} \right]^2
$$

## Topic 18: Calibration

* Poststratification: same old method, different application
* There is an alternative technique for doing poststratification when full cross-classification information is not available for the population for one or more of the poststratifying variables:
  + raking ratio estimation.
* GREG, minimizes distance criterion
$$
D(d_i, w_i) = \frac{1}{2} \sum_{i\in s} (w_i - d_i)^2 / d_i
$$
and results in weights
$$
w_{i, GREG} = d_i \left( 1 + (X - \hat X)^{\top}  T^{-1} x_i\right)
$$
where $T = \sum_{i \in s} d_i x_i x_i^\top$ and $\hat X = \sum_{i \in s} d_i x_i$, and  $X$ is a vector of known population totals for auxiliary variables

## Topic 19: Non-response bias

* Use stratification to try and make assumption of MAR plausible
* Poststratification if possible
* In the absence of population information, we can create weights based on the sample distribution:
$$
\bar y_s = \sum_h w_h \bar y_{Rh}, \quad w_h = n_h/n
$$

* Response propensity weighting
$$
w_i = \frac{1}{\hat P(R_i = 1 \mid X_i)} = \frac{1 + \exp(X_i^\top \hat \beta)}{\exp(X_i^\top \hat \beta)}
$$

## Topic 19: Non-response bias

* Imputation strategies
  + Mean imputation
  + Hot Deck imputation
  + Multiple imputation

## Topic 19: Multiple imputation

* Multiple imputation is a technique used to draw repeated samples from the posterior distribution to provide a means to estimate the additional contribution to variance due to imputation.
* Impute multiply, say $m$ times, for each missing value. (Small values - $m=5$ -- usually work well)
* Compute an estimate of interest $\bar y_{\gamma}$ for each imputed dataset, and compute
$$
T = \frac{1}{m} \sum_{\gamma} T_{\gamma}
$$

## Topic 19: Multiple imputation

* The variance of $T$ is estimated by 
$$
\begin{aligned}
\text{var} (T) &= U + \left( \frac{m+1}{m} \right) B \\
&= \frac{1}{m} \sum_{\gamma} \text{var} (T_{\gamma}) + \left( \frac{m+1}{m} \right) \left( \frac{1}{m-1} \right) \sum_{\gamma} \left( T_\gamma - T \right)^2
\end{aligned}
$$

* The distribution of T is approximated by a t-statistic with degrees of freedom given by

$$ 
df = (m-1) \left( 1 + \frac{m U}{(m+1) B} \right)^2
$$


## Topic 1: Overview

* __Population__: population denotes the aggregate from which the sample is chosen
* __Sampling frame__: Sampling units(orunits) cover population and arenon-overlapping

## Key formula:

* Population notation
* Sample notation

## Practice problem:

* What is the population and sampling frame for Pew's US polls? 

## Topic 2: SRS 

* Proved SRS w/o replacement estimator $\bar y$ is unbiased estimator of $\bar Y$
* Variance $V(\bar y)$ has a simple formula
* Estimator $s^2$ of population variance $S^2$ is unbiased 
* Estimating proportions has simpler formula since $s^2 = n p (1-p)$
* Subdomain estimation: issue is $n_j$ is random.
  + If we ignore then all formulas are the same as SRS
  + We can improve by using Taylor series (later lecture)

## Key formula:

* $V(\bar y_{SRS}) = (1-f) S^2/n$
* $s^2 = \frac{1}{n-1} \sum_{i=1}^n (y_i - \bar y)^2$
* $V(\bar y_{SRS}) = (1-f) S^2/n$
* $\bar y_j = n_j^{-1} \sum_{i=1}^n D_{ij} y_i$

## Practice problem:

* Prove SRS without replacement yields an unbiased estimator
* If I sample one individual at random, what is the variance of that estimator?
  + Can I construct an estimator of the population variance?
* Is $s$ an unbiased estimator of $S$?

## Lecture 2: Stratified sampling

* We have strata information a priori in the sampling frame
* Estimate within strata $\bar y_h$ and then estimate $\bar y = \sum_h P_h \bar y_h$
* Proportionate allocation $\Rightarrow n_h = N_h \frac{n}{N}$

## Key formula

* $S^2 \approx \sum_h P_h (S_h^2 + (\bar Y_h - \bar Y)^2)$
* $V(\bar y_{st}) = \sum_h P_h^2 (1-f_h) S_h^2/n_h$
* $V(\bar y_{prop}) = (1-f) \sum_h P_h S_h^2/n$
* $deff = V(\bar y_{st})/V(\bar y_{SRS})$
* Under prop allocation,
$$
\left(1+ \frac{\sum_h P_h (\bar Y_h - \bar Y)^2}{\sum_h P_h S_h^2} \right)^{-1}
$$
for large $N$ and $N_h$.  Not guaranteed for small sample sizes!

## Practice problem

* Two strata (men, women) with $(N_1,N_2) = (900,1200)$, $(n_1,n_2) = (10,10)$, $(\bar y_1, \bar y_2) = (10,10)$
  + Estimate the population mean
  + Let $S_1=4, S_2=2$. Compute the varaince and a 95\% confidence interval
* What if under proportionate allocation?
* What is the design effect?
  + If $(\bar Y_1, \bar Y_2) = (9,11)$


## Topic 4: Optimal allocation

* Minimize variance subject to a fixed total cost
$$
\arg \min \sum_h P_h^2 (1-f_h) \frac{S_h^2}{n_h}
$$
* Neyman allocation assumes same costs across strata

## Key formula

* Optimal: $n_h \propto P_h S_h / \sqrt{c_h}$
* Neyman: $n_h \propto P_h S_h$
* $V(\bar y_N) \approx \left( \sum_h P_h S_h \right)^2/n$
* Comparison to proportionate allocation
$$
\frac{V(\bar y_{prop}) - V(\bar y_N)}{V(\bar y_N)} \approx \frac{\sum_h P_h (S_h - \bar S)^2}{\sum_h P_h S_h^2}
$$

## Practice problem

* What is the optimal under $c_1 = 2, c_2 = 1$ and neyman allocation?
* What is the variance under neyman allocation?

## Topic 5: Poststratification

* Use auxiliary information $\bar y_{ps} = \sum_h P_h \bar y_h$
* The denominator is now random!
* Impacts the variance (larger) when compared to stratified sampling
* Subdomain estimation with a stratified sample
  + Use the taylor series arguments again

## Key formula

$$
V(\bar y_{ps}) \approx V(\bar y_{st}) + (1-f) \sum_h \frac{S_h^2(1-P_h)}{n^2}
$$

## Topic 6: Equal-sized cluster sampling

* Mean is an unbiased estimate
* $V(\bar y) = \frac{K-k}{K} \frac{1}{k} \frac{1}{K-1} \sum_i (\bar Y_i - \bar Y)^2$
* We can approximetly express as
$$
V(\bar y_c) = V(\bar y_{SRS}) [1+(M-1)\rho]
$$
* Design effect easy to estimate this way (but is approximate!)
* CI depends on t-statistic with $k-1$ df

## Topic 7: Systematic sampling

* Choose a starting point, the only randomness is in this choice
* The mean is unbiased
* Cluster sampling with only one cluster chosen $\Rightarrow$ problems!!
$$
V(\bar y_c) = \frac{S^2}{n} \frac{nk-1}{nk} [1 + (n-1) \rho]
$$
* Option 1: assuming a random order then $\rho = 0$
* Option 2: Paired selections model
$$
V(\bar y_{sys} ) = \frac{1-f}{n^2} \sum_{j=1}^{n/2} \frac{(y_{j1}-y_{j2})^2}{2}
$$
* Option 3: Successive differences model
$$
V(\bar y_{sys} ) = \frac{1-f}{2 n(n-1)} \sum_{j=1}^{n-1} (y_{j}-y_{j+1})^2
$$

## Topic 8: Multi-stage cluster sampling

* Mean is an unbiased estimate
* $V(\bar y) = (1-k/K) S_1^2/k + (1-m/M) S_2^2/n$, where 
$$
S_1^2 = \frac{\sum_i (\bar Y_i - \bar Y)^2}{K-1} \quad
S_2^2 = \frac{\sum_i \sum_j (Y_{ij} - \bar Y_i)^2}{K(M-1)} \quad
$$
* $s_2^2$ unbiased estimator but need an unbiased estimator of $S_1^2$ is 
$$
s_1^2 - (1-m/M) s_2^2/m
$$
* Variance is then $(1-f_1) s_1^2/k + f_1 (1-f_2) s_2^2/n$
* Design effect is 
$$
[1+(m-1)\rho]-(M-m) \frac{f}{1-f} \rho
$$

## Topic 8: Multi-stage cluster sampling

* 3-stage sampling
* $V(\bar y) = (1-k/K) S_1^2/k + (1-m/M) S_2^2/km + (1-r/R)S_3^2/kmr$, where 
$$
S_2^2 = \frac{\sum_i \sum_j (\bar Y_{ij} - \bar Y_i)^2}{K(M-1)} \quad
S_2^2 = \frac{\sum_i \sum_j (\bar Y_{ijl} - \bar Y_{ij})^2}{KM(R-1)} \quad
$$
* Need an unbiased estimator of $S_3^2$ is 
$$
\hat s_1^2 - (1-m/M) \hat S_2^2/m - (1-r/R) \hat S_3^2/mr
$$
* Variance is then $(1-f_1) s_1^2/k + f_1 (1-f_2) s_2^2/km + f_1 f_2 (1-f_3)s_3^2/n$

## Topic 9: Optimal cluster size

* 2-stage sampling: minimize $\Psi = V(\bar y_c) \cdot C$ then
$$
m = \sqrt{\frac{c_1(1-\rho)}{c_2 \rho}} \quad
= \sqrt{\frac{c_1S_2^2}{c_2 S_u^2}} \text{ where } S_u^2 = S_1^2 - S_2^2/M
$$

## Topic 10: Ratio estimator

* We have $R = \bar Y/ \bar X$ which allows us to estimate $\bar Y = R \bar X$ using $\bar y_{R} = r \bar X$
* Biggest issue is $\bar x$ is random!
* Variance is approximately given by
$$
\frac{1-f}{n \bar X^2} \left[ S_y^2 - R^2 S_x^2 - 2 R S_{xy} \right]
$$
* Good option when 
$$
S_{xy} > \frac{1}{2} R S_x^2 \quad \text{ or } \quad
\rho > \frac{1}{2} \frac{cv(\bar x)}{cv(\bar y)}
$$
where $cv(\bar x) = S_x / \bar X$

## Topic 11: Regression estimator

* Regression estimator $\bar y_{lr} = \bar y + b (\bar X - \bar x)$ where
$$
b = \frac{\sum_i (y_i - \bar y)(x_i - \bar x)}{\sum_i (x_i - \bar x)^2 }
$$
* Variance of estimator is 
$$
\frac{1-f}{n} \left( S_y^2 - 2 B S_{yx} + B^2 S_x^2 \right)
\Rightarrow
\frac{1-f}{n} S_y^2 \left( 1 - \rho^2 \right)
$$


## Practice Problems

- See announcement for link to practice problems
- The next few slides outline solutions for reference

## Problem 1

For a population of N = 1000, two strata are formed. The aim is to estimate the mean of a characteristic $Y$, $\bar Y$ . From a previous year, he entire population was examined, yielding the following results:

\begin{table}[!th]
\centering
\begin{tabular}{c c c c}
\hline
Stratum & $P_h$ & $\bar Y_h$ & $S_h^2$ \\ \hline
1 & 0.9 & 15 & 16 \\
2 & 0.1 & 20 & 25 \\ \hline
\end{tabular}
\end{table}

## Problem 1a:

* Calculate the variance $S^2$ and for a SRS size $n=100$, the sampling variance of the sample mean $\bar y$
* $S^2 = \sum_h P_h (S_h^2 + (Y_h - \bar Y)^2 ) = 19.15$
* $V(\bar y) = \frac{1-f}{n} S^2 = 0.1724$

## Problem 1b:

* For a stratified random sample of size $n=100$ allocated proportionately, what will be the sampling variance of the sample mean?

$$ V(\bar y_{st} ) = \frac{1-f}{n} \sum_h P_h S_h^2 = 0.1521 $$

## Problem 1c:

* What is the optimal (here assuming constant costs, so Neyman) allocation and sampling variance of the sample mean for a sample size $n = 10$?
* What is the gain in precision relative to SRS from the optimal allocation in above?
* When will $D^2 (\bar y_{opt}) < D^2 (\bar y_{prop})$?


## Problem 2a:

Show that the design-based estimator of a linear regression parameter estimate for the model $E(Y) = X \beta$ under a SRS of size $n$ drawn from a population of size $N$ is given by the usual linear regression parameter estimator $\hat \beta = (X^\top X)^{-1} X^\top Y$

* Since $w_i = \frac{N}{n}$ for all $i$, then $(X^\top W X)^{-1} =\frac{n}{N} \left( X^\top X \right)^{-1}$
* Then $X^\top W Y = \frac{N}{n} X^\top Y$
* And thus $\hat \beta = (X^\top W X)^{-1} (X^\top W Y ) = (X^\top X)^{-1} X^\top Y$

## Problem 2b:

Show that the design-based estimator of the variance-covariance matrix for $\hat \beta$ in (a) is given by
$$
v(\hat \beta) = \frac{n}{n-1} \left( X^\top X \right)^{-1} \left( X^\top E E X \right) \left( X^\top X \right)^{-1}
$$
where $E = \text{diag} (e_i)$ for residuals $e_i =  y_i - \hat y_i = y_i - x_i^\top \hat \beta$. Suppose the model were truly linear, i.e., $y_i = x_i^\top \beta + e_i$ for $e_i \sim N(0,\sigma^2)$.  What is the expectation of $e_i^2$?  Plug this in for $E \cdot E$, and what do you get?

## Problem 2b:

* As above $(X^\top W X)^{-1} = \frac{n}{N} (X^\top X)^{-1}$; then
$$
\sum_{i=1}^h \frac{k_i}{k_i-1} \sum_{j=1}^k (z_{ij} - \bar z_i) (z_{ij} - \bar z_i)^\top = 
\frac{n}{n-1} \sum_{i=1}^n (z_{i} - \bar z) (z_{i} - \bar z)^\top 
$$
for $\bar z = \frac{1}{n} \sum_{j=1}^n z_j$.

* Let $v_j = e_j x_j$ so that $Z_j = \frac{N}{n} v_j$; then $\bar z = \frac{N}{n^2} \sum_{j=1}^n v_j= 0$ since $\sum_{j=1}^n v_j = 0$, and
$$
\begin{aligned}
\frac{n}{n-1} \sum_{i=1}^n (z_{i} - \bar z) (z_{i} - \bar z)^\top &= \frac{n}{n-1} \sum_j z_j z_j^\top \\
&= \frac{n}{n-1} \left( \frac{N}{n} \right)^2 \sum_j v_j v_j^\top \\
&= \frac{n}{n-1} \left( \frac{N}{n} \right)^2 V^\top V
\end{aligned}
$$

## Problem 2b:

Then 
$$
\begin{aligned}
v(\hat \beta) &= \frac{n}{n-1} (X^\top X)^{-1} (V^\top V) (X^\top X)^{-1} \\
&= \frac{n}{n-1} (X^\top X)^{-1} (X^\top E E X) (X^\top X)^{-1} 
\end{aligned}
$$
since $V = E X$ and $E^\top = E$.

* By the model we have $e_i/\sigma \sim N(0,1)$ and therefore $e_i^2/ \sigma^2 \sim \chi^2_1$ and so $E[e_i^2/\sigma^2] = 1$.  So this means we replace $E E$ with $\text{diag}(\sigma^2)$ and we recover the model-based variance formula.

## Problem 3:

We considered two descriptions of non-response bias: deterministic, where each subject is assumed to have a response indicator $R_i=1$ if the respond and $R_i = 0$ if they do not respond, and stochastic, with a probability of response $0 \leq P_i \leq 1$, with the bias of the respondent mean given as follows:

* Deterministic: $B(\bar y_r) = (1-R) (\bar Y_R - \bar Y_M)$ where $R$ is the proportion of respondents in the population.
* Stochastic: $B( \bar y_r ) = \frac{1}{\bar P} C(Y, P)$ and $\bar P = \frac{1}{N} \sum_{i=1}^N P_i$.
* Show that, in the limiting stochastic case where $P_i = 0$ or $P_i = 1$, the stochastic bias corresponds to the deterministic bias
* In the stochastic setting, suppose that there are $H$ strata and the probability of response depends on strata.  Re-write the bias including terms $P_h$ and $\bar Y_h$.

## Problem 3

$$
\begin{aligned}
B(\bar y_r) &= \frac{1}{R} \frac{1}{N} \sum_i (Y_i - \bar Y) (P_i - R) \\
&=  \frac{1}{R \times N} \sum_i (Y_i - \bar Y) (1 - R) 1[R_i = 1] \\
&- \frac{1}{R \times N} \sum_i (Y_i - \bar Y) R 1[R_i = 0] \\
&= (1-R) \bar Y_r - (1-R) \bar Y - (1-R) \bar Y_M + \bar Y (1-R) \\
&= (\bar Y_r - \bar Y_m) (1-R)
\end{aligned}
$$

## Problem 3:

$$
\begin{aligned}
B(\bar y_r) &= \frac{1}{\bar P} \frac{1}{N} \sum_i (Y_i - \bar Y) (P_i - \bar P)  \\
&= \frac{1}{\sum_h \frac{N_h}{N} P_h } \sum_{h=1}^H \frac{N_h}{N} (P_h - \bar P) (\bar Y_h - \bar Y) 
\end{aligned}
$$
	
## Problem 4

 Consider following SRS sample, tabulated by gender and age:
\begin{table}[!th]
\centering
\begin{tabular}{c c c}
& \multicolumn{2}{c}{Age Group} \\ \cline{2-3}
Gender & $\leq 25$ years & $> 25$ years \\ \hline
Female & 17 & 16 \\
Male & 40 & 31 \\ \hline
\end{tabular}
\end{table}

With the known population data

\begin{table}[!th]
\centering
\begin{tabular}{c c c}
& \multicolumn{2}{c}{Age Group} \\ \cline{2-3}
Gender & $\leq 25$ years & $> 25$ years \\ \hline
Female & 46 & 150 \\
Male & 200 & 400 \\ \hline
\end{tabular}
\end{table}

## Problem 4

And means per group:

\begin{table}[!th]
\centering
\begin{tabular}{c c c}
& \multicolumn{2}{c}{Age Group} \\ \cline{2-3}
Gender & $\leq 25$ years & $> 25$ years \\ \hline
Female & 40 & 57 \\
Male & 29 & 77 \\ \hline
\end{tabular}
\end{table}

* Construct the postratification mean estimate
* Suppose that the number of respondents per strata was actually

\begin{table}[!th]
\centering
\begin{tabular}{c c c}
& \multicolumn{2}{c}{Age Group} \\ \cline{2-3}
Gender & $\leq 25$ years & $> 25$ years \\ \hline
Female & 13 & 11 \\
Male & 30 & 25 \\ \hline
\end{tabular}
\end{table}

How would you use postratification weights in this setting? How is this different from first part?

## Problem 4: Simple poststratification

```{r problem4, echo = FALSE, eval = TRUE}
n = c(17,16,40,31)
psw = c(46,150,200,400)
psw_2 = c(100,125,300,200)
meany = c(40,57,29,77)
response = c(13,11,30,25)
response_rate = response/sum(response)
```

* Poststratification estimate is 
$$
\sum_h P_h \bar y_h = \frac{46}{796} \times 40 + \frac{150}{796} \times 57 + \frac{200}{796} \times 29 + \frac{400}{796} \times 77 = `r round(sum(psw*meany)/sum(psw),2)`
$$

* If we want to write in the weighted method, then $w_h^{PS} = N_h/n_h$ and 
$$
\begin{aligned}
\frac{\sum_h \sum_i w_{hi}^{PS} \cdot y_{hi}}{\sum_h \sum_i w_{hi}^{PS}} 
&= \frac{\sum_h \frac{N_h}{n_h} n_h \bar y_h }{\sum_h n_h \frac{N_h}{n_h} } \\
&= \frac{\sum_h N_h \bar y_h }{\sum_h N_h } = \sum_h P_h \bar y_h.
\end{aligned}
$$

## Problem 4: Non-response
* When we adjust for response status, the weight becomes $N_h / N / r_h / r$.

$$
\frac{\sum_h (N_h/N)/(r_h/r) r_h \bar y_h}{\sum_h (N_h/N)/(r_h/r) r_h } = `r round(sum(psw/response_rate*response*meany)/sum(psw/response_rate*response),2)`
$$

## Problem 4

Suppose you had a different population with data

\begin{table}[!th]
\centering
\begin{tabular}{c c c}
& \multicolumn{2}{c}{Age Group} \\ \cline{2-3}
Gender & $\leq 25$ years & $> 25$ years \\ \hline
Female & 100 & 125 \\
Male & 300 & 200 \\ \hline
\end{tabular}
\end{table}

and would like to transport the effect.  Use poststratification weights to transport the effect under both complete-case and non-response settings.  Under what assumption do these methods work well?

## Problem 4: 

* We apply poststratification with weights $N_h/N$ constructed from the new population data
$$
\sum_h (N_h/N) \bar y_h =  `r round(sum(psw_2*meany)/sum(psw_2),2)`
$$

* When we adjust for response status, the weight becomes $N_h / N / r_h / r$.

$$
\frac{\sum_h (N_h/N)/(r_h/r) r_h \bar y_h}{\sum_h (N_h/N)/(r_h/r) r_h } = `r round(sum(psw_2/response_rate*response*meany)/sum(psw_2/response_rate*response),2)`
$$

