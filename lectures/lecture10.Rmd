---
title: "BIOS 617 - Lecture 10"
author: "Walter Dempsey"
date: "2/8/2023"
output:
  beamer_presentation: default
  ioslides_presentation: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```
## Mutli-stage sampling

```{r, out.width = "250px", fig.align='center'}
library(knitr)
include_graphics("./figs/l8_fig1.png") # place holder
```

## Multi-stage sampling

```{r, out.width = "250px", fig.align='center'}
library(knitr)
include_graphics("./figs/l8_fig2.png") # place holder
```

## Sampling variance of mean from two-stage cluster sample

$$
\begin{aligned}
V(\bar y_c) = \left( 1 - \frac{k}{K} \right) \frac{S_1^2}{k} + \left( 1 - \frac{m}{M} \right) \frac{S_2^2}{n}
\end{aligned}
$$
where
$$
S_1^2 = \frac{\sum_{i=1}^{K} (\bar Y_i - \bar Y)^2}{K-1} 
\quad \text{ and } \quad
S_2^2 = \frac{\sum_{i=1}^{K} \sum_{j=1}^M (Y_{ij} - \bar Y_i)^2}{K(M-1)} 
$$

* This should be intuitive, but needs to be proven. 
* Key idea: treat as proportionately stratified sample and use
$$
V(\bar y_c) = E \left[ V(\bar y_c \mid i \in s ) \right] + V \left( E[ \bar y_c \mid i \in s ] \right)
$$
where treat chosen clusters as strata since we are fixing the clusters through conditioning.

## Unbiased estimators

* Replace $S_2^2$ with $s_2^2$ and replace $S_1^2$ with $s_1^2 - \left( 1- \frac{m}{M} \right) \frac{s_2^2}{m}$
yields an unbiased estimator of $V(\bar y_c)$.

$$
\begin{aligned}
v(\bar y_c ) &= \left( 1 - \frac{k}{K} \right) \frac{1}{k} \left(s_1^2 - \left( 1- \frac{m}{M} \right) \frac{s_2^2}{m} \right) + \left( 1 - \frac{m}{M} \right) \frac{s_2^2}{km} \\
&= \left( 1 - \frac{k}{K} \right) \frac{s_1^2}{k} + \left( 1 - \frac{m}{M} \right) \frac{s_2^2}{km} \left( 1 - \left( 1 - \frac{k}{K} \right) \right) \\
&= \left( 1 - \frac{k}{K} \right) \frac{s_1^2}{k} + \frac{k}{K} \left( 1 - \frac{m}{M} \right) \frac{s_2^2}{km}  \\
&= (1-f_1) \frac{s_1^2}{k} + f_1 (1-f_2) \frac{s_2^2}{n}
\end{aligned}
$$

## Design effect for 2-stage sampling

From previous results we have
$$
\frac{N-1}{N} S^2 = \frac{K-1}{K} S_1^2 + \frac{M-1}{M} S_2^2
$$
and
$$
\rho = \frac{\sum_{i=1}^K \sum_{j \neq l} (Y_{ij} - \bar Y) (Y_{il} - \bar Y) }{(N-1)(M-1)S^2}
$$

## Re-writing cross term 

$$
\begin{aligned}
\sum_{j \neq l} (Y_{ij} - \bar Y) (Y_{il} - \bar Y) &= \sum_{j,l=1}^M (Y_{ij} - \bar Y) (Y_{il} - \bar Y) - \sum_{j=1}^M (Y_{ij} - \bar Y)^2 \\
&= \left[ \sum_{j=1}^M (Y_{ij} - \bar Y) \right]^2 - \sum_{j=1}^M (Y_{ij} - \bar Y)^2 \\
&= \left[ M ( \bar Y_{i} - \bar Y) \right]^2 - \sum_{j=1}^M (Y_{ij} - \bar Y)^2 \\
\end{aligned}
$$

## And thus

$$
\begin{aligned}
&\sum_{i=1}^K \sum_{j \neq l} (Y_{ij} - \bar Y) (Y_{il} - \bar Y) \\
=& M^2 \sum_{i=1}^K (\bar Y_i - \bar Y)^2 - \sum_{i=1}^K \sum_{j=1}^M (Y_{ij} - \bar Y)^2 \\
=& M^2 (K-1) S_1^2 - (N-1) S^2 \\
=& M^2 (K-1) S_1^2 - [M(K-1)S_1^2 + K(M-1) S_2^2] \\
=& M(K-1)(M-1) S_1^2 - K(M-1)S_2^2
\end{aligned}
$$

## Plug and play

$$
\rho = \frac{\sum_{i=1}^K \sum_{j \neq l} (Y_{ij} - \bar Y) (Y_{il} - \bar Y) }{(N-1)(M-1)S^2}
= \frac{\frac{K-1}{K}S_1^2 - \frac{S_2^2}{M}}{\frac{N-1}{N} S^2} \to \frac{S_1^2 - \frac{S_2^2}{M}}{S^2}
$$
As $K,N \to \infty$. Using the previous result that $S_w^2 = S_2^2 \approx (1-\rho) S^2$, we have
$$
S_1^2 = \rho S^2 + \frac{(1-\rho) S^2}{M} = S^2 \left[ \frac{1}{M} + \rho \left( 1 - \frac{1}{M} \right) \right]
= S^2 [1+(M-1)\rho]/M
$$

## Plugging this back into $V(\bar y_c)$

$$
\begin{aligned}
V(\bar y_c) &= \left( 1  - \frac{k}{K} \right) \frac{S_1^2}{k} + \left( 1  - \frac{m}{M} \right) \frac{S_2^2}{k}  \\
&\approx \left( 1  - \frac{k}{K} \right) \frac{1}{k} \left( \frac{S^2}{M} (1+(M-1)\rho) \right) + \left( 1  - \frac{m}{M} \right) \frac{(1-\rho) S^2}{k}  \\
&= \frac{S^2}{n} \left[ \left( 1 - \frac{k}{K} \right) \left( \frac{m}{M} ((1-\rho)+M\rho) + (1-\rho) - \frac{m}{M} (1-\rho) \right) \right] \\
&= \frac{S^2}{n} \left[ \left( 1 - \frac{k}{K} \right) m \rho - \frac{n}{N} (1-\rho) + (1-\rho) \right] \\
&= \frac{S^2}{n} \left[ \left( 1 - \frac{n}{N} \right) + \left( m - 1 - \frac{n}{N} (M-1) \right) \rho \right] \\
&= \frac{S^2}{n} \left[ \left( 1 - \frac{n}{N} \right) + \left( 1 - \frac{n}{N} \right)(m-1)\rho + \frac{n}{N} \left( m - 1 - (M-1) \right) \rho \right] \\
&= (1-f)\frac{S^2}{n} [1+(m-1)\rho] - \frac{M-m}{N} S^2 \rho.
\end{aligned}
$$

##  Design effect

* The design effect associated with a mean from two-stage cluster sample of equal cluster size is given by 
$$
deff = [1+(m-1)\rho] - (M-m) \frac{f}{1-f} \rho
$$
or just $1+(m-1)\rho$ as $K$ and thus $N \to \infty$ for a fixed $M$.
* Note that $m$ (unlike $\rho$) can be manipulated by the design, so the design effect can be controlled.


## NEW MATERIAL: 3-stage sampling

```{r, out.width = "250px", fig.align='center'}
library(knitr)
include_graphics("./figs/l9_fig1.png") # place holder
```


## 3-stage sampling

```{r, out.width = "250px", fig.align='center'}
library(knitr)
include_graphics("./figs/l9_fig2.png") # place holder
```

## Notation

* Select equal elements at second ($m$) __and__ at third stage ($r$)
* Population mean: 
$$
\bar Y = \frac{1}{N} \sum_{i=1}^K \sum_{j=1}^M \sum_{l=1}^R Y_{ijl} = \frac{1}{KM} \sum_{i=1}^K \sum_{j=1}^M \bar Y_{ij} = \frac{1}{K} \sum_{i=1}^K \bar Y_i
$$
* Sample mean
$$
\bar y = \frac{1}{n} \sum_{i=1}^k \sum_{j=1}^m \sum_{l=1}^r y_{ijl} = \frac{1}{km} \sum_{i=1}^k \sum_{j=1}^m \bar y_{ij} = \frac{1}{k} \sum_{i=1}^k \bar y_i
$$
* Sampling fraction
$$\
\begin{aligned}
P(i \in s ) P(j \in s \mid i \in s ) P(l \in s \mid j \in s, i \in s ) &= \frac{k}{K} \times \frac{m}{M} \times \frac{r}{R} \\
&= \frac{n}{N} = f
\end{aligned}
$$

## Variance $V(\bar y_c)$

$$
\left( 1- \frac{k}{K} \right) \frac{S_1^2}{k} + 
\left( 1- \frac{m}{M} \right) \frac{S_2^2}{km} +
\left( 1- \frac{r}{R} \right) \frac{S_3^2}{kmr}
$$
where
$$
\begin{aligned}
S_1^2 &= \frac{\sum_{i=1}^K (\bar Y_i - \bar Y)^2}{K-1}, \\
S_2^2 &= \frac{\sum_{i=1}^K \sum_{j=1}^M (\bar Y_{ij} - \bar Y_i)^2}{K(M-1)}, \\
S_3^2 &= \frac{\sum_{i=1}^K \sum_{j=1}^M \sum_{l=1}^R (Y_{ijl} - \bar Y_{ijl})^2}{KM(R-1)},
\end{aligned}
$$

## Variance proof

$$
\begin{aligned}
V( \bar y_c ) &= E ( V(\bar y_c \mid i \in s) ) + V( E (\bar y_c \mid i \in s)) \\
&= \underbrace{E ( E ( V ( \bar y_c  \mid i \in s, j \in s) \mid i \in s ))}_{(1)}  \\
&+ \underbrace{E(V ( E ( \bar y_c \mid i \in s, j \in s) \mid i \in s ) )}_{(2)} \\
&+ \underbrace{V( E ( E (\bar y_c) \mid i \in s, j \in s) \mid i \in s)}_{(3)} \\
\end{aligned}
$$

## Term 3

$$
E( E ( \bar y_c \mid i \in s, j \in s) \mid i \in s ) = E (\bar y_c \mid i \in s ) = \frac{1}{k} \sum_{i=1}^k \bar Y_i
$$
so we can directly apply previous results
$$
V( E ( E ( \bar y_c \mid i \in s, j \in s) \mid i \in s)) = \left( 1 - \frac{k}{K}\right) \frac{S_1^2}{k}
$$

## Term 2

$$
E ( V ( E ( \bar y_c \mid i \in s, j \in s ) \mid i \in s) ) = E \left( v \left( \frac{1}{km} \sum_{i=1}^k \sum_{j=1}^m \bar Y_{ij} \right ) \mid i \in s \right)
$$
so from two-stage work, we know this is equal to 
$$ 
E \left( v \left( \frac{1}{km} \sum_{i=1}^k \sum_{j=1}^m \bar Y_{ij} \right ) \mid i \in s \right) =
\left( 1 - \frac{m}{M} \right) \frac{S_2^2}{km}
$$

## Term 1

Using results from proportionally stratified sampling design
$$
V( \bar y_c \mid i \in s, j \in s) = \left( 1 - \frac{r}{R} \right) \frac{1}{km} \sum_{i=1}^k \sum_{j=1}^m \sum_{l=1}^r \frac{\left( Y_{ijl} - \bar Y_{ij} \right)^2}{r-1}
$$
so 
$$
E( E ( V(\bar y_c \mid i \in s, j \in s) \mid i \in s)) = \left( 1- \frac{r}{R} \right) \frac{S_3^2}{n}
$$
where
$$
S_3^2 = \frac{\sum_{i=1}^K \sum_{j=1}^M \sum_{l=1}^R (Y_{ijl} - \bar Y_{il})^2}{KM(R-1)},
$$

## Estimating sampling variance

* Need to estimate $S_1^2, S_2^2, S_3^2$
* Using similar work, we have
  + $s_3^2 = \frac{\sum_{i=1}^k \sum_{j=1}^m \sum_{l=1}^r (y_{ijl} - \bar y_{ij})^2}{km(r-1)}$ is unbiased estimator of $S_3^2$
  + $s_2^2 - \left( 1- \frac{r}{R} \right) \frac{s_3^2}{r}$ is unbiased estimator of $S_2^2$
* Then what is the bias for
$$
s_1^2 = \frac{\sum_{i=1}^k (\bar y_i - \bar y)^2}{k-1}
$$
* Suffices to study 
$$
\sum_{i=1}^k (\bar y_i - \bar y)^2 = \sum_{i=1}^k \bar y_i^2 - k \bar y^2
$$
  
## What about $S_1^2$?

$$
\begin{aligned}
E \left(\sum_{i=1}^k \bar y_i^2 \right) &= E \left( E \left( E \left ( \sum_{i=1}^k \bar y_i^2 \mid i \in s, j \in s \right) \mid i \in s \right) \right) \\
&= E \left( \sum_{i=1}^k V(\bar y_i) \right) + E \left( \sum_{i=1}^k \bar Y_i^2 \right) \\
&= \underbrace{E \left( \sum_{i=1}^k \left[ \left( 1- \frac{m}{M} \right) \frac{S_{2i}^2}{km} + \left( 1 - \frac{r}{R} \right) \frac{S_{3i}^2}{n} \right] \right)}_{\text{from 2-stage results}} + \frac{k}{K} \sum_{i=1}^K \bar Y_i^2  \\
&= k \left[ \left( 1 - \frac{m}{M} \right) \frac{S_2^2}{m} + \left( 1 - \frac{r}{R} \right) \frac{S_3^2}{mr} \right] + \frac{k}{K} \sum_{i=1}^K \bar Y_i^2.
\end{aligned}
$$

## What about $S_1^2$?

$$
\begin{aligned}
E(\bar y^2) &= V(\bar y) + E(\bar y)^2 \\
&=\left( 1- \frac{k}{K} \right) \frac{S_1^2}{k} + 
\left( 1- \frac{m}{M} \right) \frac{S_2^2}{km} +
\left( 1- \frac{r}{R} \right) \frac{S_3^2}{kmr} + \bar Y^2
\end{aligned}
$$

## Combine and conquer

$$
\begin{aligned}
E(\sum_{i=1}^k \bar y_i^2 - k \bar y^2) =&  k \left[ \left( 1 - \frac{m}{M} \right) \frac{S_2^2}{m} + \left( 1 - \frac{r}{R} \right) \frac{S_3^2}{mr} \right] + \frac{k}{K} \sum_{i=1}^K \bar Y_i^2 \\
&-k \bigg[ \left( 1- \frac{k}{K} \right) \frac{S_1^2}{k} + 
\left( 1- \frac{m}{M} \right) \frac{S_2^2}{km} \\
&+ \left( 1- \frac{r}{R} \right) \frac{S_3^2}{kmr} + \bar Y^2 \bigg] \\
&= (k-1) \left[ S_1^2 + \left(  1- \frac{m}{M} \right) \frac{S_2^2}{m} + \left( 1 - \frac{r}{R} \right) \frac{S_3^2}{mr}  \right]
\end{aligned}
$$

## Estimator of $S_1^2$

$$
\hat s_1^2 - \left(  1- \frac{m}{M} \right) \frac{ \hat S_2^2}{m} - \left( 1 - \frac{r}{R} \right) \frac{\hat S_3^2}{mr}  
$$
which is equal to 
$$
\hat s_1^2 - \left(  1- \frac{m}{M} \right) \left( \frac{ \hat s_2^2}{m} - \left( 1 - \frac{r}{R} \right) \frac{ \hat s_3^2}{mr} \right) - \left( 1 - \frac{r}{R} \right) \frac{\hat s_3^2}{mr}  
$$

## Put it all together and what do you get

$$ 
v(\bar y_c) = (1-f_1) \frac{s_1^2}{k} + f_1 ( 1-f_2) \frac{s_2^2}{km} + f_1 f_2 \left ( 1 - f_3 \right) \frac{s_3^2}{n}
$$

* Note that when the PSU sampling fraction $f_1 = \frac{k}{K}$ is small enough, the additional terms due to the uncertainty in the later stage estimation become trivial compared to the first term, so that $v(\bar y_c) \approx s_1^2/k$ regardless of how many stages there are!
* Sometimes termed ultimate cluster design, since all of the lower levels are
aggregated into the highest level (“ultimate”) cluster and treated as if they
came from a one-stage design with a census in each cluster.

## Optimal cluster size (2-stage sampling)
* Let $c_1$ be the average cost to recruit a cluster into a sample and $c_2$ be the average cost to recruit and conduct an interview at a sampled unit
  + For a face-to-face area probability sample, $c_1$ might include the costs of developing and maintaining an interviewer or interviewer team in a sampled PSU, while $c_2$ includes the costs of traveling to sampled households, recruiting respondents, and conducting interviews.
  + For a school survey, $c_1$ might include the costs of recruiting a sampled school, obtaining institutional review, etc., while $c_2$ includes the per-survey costs of administering and processing the surveys
  
## Optimal cluster size

Not including any overall fixed costs $C_0$, the total cost of conducting a two-stage survey will be:
$$
C = k c_1 + n c_2 = n (c_2 + c_1 \cdot k / n) = n (c_2 + c_1 / m)
$$
We will focus on an optimization of the product of the variance of the mean and the cost:
$$
\Psi = V(\bar y_c) \cdot C
$$

## First attempt

Consider the approximation $V(\bar y_c) \approx \frac{S^2}{n} \left[ 1+ (m-1) \rho \right]$ then
$$
\Psi = \frac{S^2}{n} \left[ 1+ (m-1) \rho \right] \cdot \left( n (c_2 + c_1/m) \right)
$$
Then
$$
\frac{\partial \Psi}{\partial m} = S^2 \left[ c_2 \rho - \frac{c_1 (1-\rho)}{m^2} \right] = 0
$$
so
$$
m = \sqrt{\frac{c_1 (1-\rho)}{c_2 \rho}}
$$

## Exact result

$$
\begin{aligned}
V(\bar y_c) &= \left( 1 - \frac{k}{K} \right) \frac{S_1^2}{k} + \left( 1 - \frac{m}{M} \right) \frac{S_2^2}{n} \\
&= \frac{1}{k} \underbrace{\left( S_1^2 - \frac{S_2^2}{M} \right)}_{=S_u^2} + \frac{S_2^2}{n} - \frac{S_1^2}{K}
\end{aligned}
$$

## Keep going

Ignoring the last term since it doesn't involve manipulable variables of sample size, we have
$$
\begin{aligned}
\Psi &= \left[\frac{S_u^2}{k}  + \frac{S_2^2}{n}\right] \left[ n (c_2 + c_1/m) \right] \\
&= \left[ m S_u^2  + S_2^2 \right] \left[ (c_2 + c_1/m) \right] \\
&= m S_u^2 c_2 + S_2^2 c_2 + c_1 S_u^2 + c_1 S_2^2/m
\end{aligned}
$$
Therefore
$$
\frac{\partial \Psi}{\partial m} = S_u^2 c_2 - c_1 S_2^2/m^2 = 0
$$
implying
$$
m = \sqrt{\frac{c_1 S_2^2}{c_2 S_u^2}}
$$

## What about $k$?

* Once $m$ is determined, $k$ is obtained from the cost function:
$$
C = n (c_2 + c_1/m) = km (c_2 + c_1 / m)
$$
so that
$$
k = \frac{C}{m c_2+c_1}
$$
Note that
* Optimal value of $m$ depends only on the relative per-cluster and per-unit costs and the between-cluster vs. within-cluster variance (i.e., intraclass correlation).
* Once $m$ is determined, the number of clusters $k$ to be sampled is simply a function of the number of PSUs to be selected.

## 3-stage sampling

Now our cost function is 
$$
C = \underbrace{k}_{=n_1}c_1 + \underbrace{km}_{=n_2} c_2 + \underbrace{kmr}_{=n_3} c_3 = \sum_i c_i n_i
$$
where $c_1$ and $c_2$ are the costs associated with obtaining and organizing data collection at the 1st and 2nd stage clusters respectively, and $c_3$ is now the average cost to recruit and conduct an interview at a sampled unit.

## 3-stage sampling

Our 3-stage variance is given by
$$
\begin{aligned}
V(\bar y_c) &= \left( 1 - \frac{k}{K} \right) \frac{S_1^2}{k} + \left( 1 - \frac{m}{M} \right) \frac{S_2^2}{km} + \left( 1 - \frac{r}{R} \right) \frac{S_3^2}{n} \\
&= \frac{S_1^2}{n_1} + \frac{S_2^2}{n_2} + \frac{S_3^2}{n_3} - \frac{S_1^2}{K} - \frac{S_2^2}{n_1 M} - \frac{S_3^2}{n_2 R} \\
&= \frac{1}{n_1} \left( S_1^2 - \frac{S_2^2}{M} \right) + \frac{1}{n_2} \left( S_2^2 - \frac{S_3^2}{R} \right) + \frac{S_3^2}{n_3} - \frac{S_1^2}{K} \\
&= \sum_i V_i / n_i - V_0
\end{aligned}
$$
where $V_0 = \frac{S_1^2}{K}$, $V_1 = S_1^2-\frac{S_2^2}{M}$, $V_2 = S_2^2 - \frac{S_3^2}{R}$, and $V_3 = S_3^2$.

## 3-stage sampling

Ignoring $V_0$, the optimizing function is given by
$$
\Psi = \left[ \sum_i V_i / n_i \right] \cdot \left [ \sum_i c_i n \right]
$$

* Could obtain derivatives with respect to $n_i$, but can also ``cheat'' by noting that for the Cauchy-Schwartz inequality 
$$
\sum_i a_i^2 \sum_i b_i^2 \geq \left( \sum_i a_i b_i \right)^2
$$
equality holds when $a_i \propto b_i$, or $a_i / b_i$ is a constant.
* Thus $\Psi$ is minimized when 
$$
\frac{\sqrt{V_i/n_i}}{\sqrt{c_i n_i}} = \frac{1}{n_i} \sqrt{\frac{V_i}{c_i}}
$$
or $n_i \propto \sqrt{V_i / c_i}$.

## 3-stage sampling

$$
\frac{n_3}{n_2} = r = \sqrt{ \frac{V_3}{V_2} \times \frac{c_2}{c_3}} = \sqrt{ \frac{S_3^2}{(S_2^2 - S_3^2/R)} \times \frac{c_2}{c_3}}
$$
$$
\frac{n_2}{n_1} = m = \sqrt{ \frac{V_2}{V_1} \times \frac{c_1}{c_2}} = \sqrt{ \frac{S_2^2-S_3^2/R}{(S_1^2 - S_2^2/M)} \times \frac{c_1}{c_2}}
$$
With $m$ and $r$ determined, we can then find $k$:
$$
\begin{aligned}
C &= k c_1 + k m c_2 + k m r c_3\\
\Rightarrow k &= \frac{C}{c_1 + m c_2 + mr c_3}
\end{aligned}
$$

